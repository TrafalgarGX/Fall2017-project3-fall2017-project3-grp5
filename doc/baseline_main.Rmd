---
title: "baseline_main"
author: "Yufei Zhao"
date: "2017/10/29"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
print(R.version)
```

```{r results='hide', message=FALSE, warning=FALSE}
if(!require("gbm")){
  install.packages("gbm")
}

library(gbm)
```



### Step 1: set up controls for evaluation experiments.

In this chunk, ,we have a set of controls for the evaluation experiments. 

+ (T/F) cross-validation on the training set
+ (number) K, the number of CV folds
+ (T/F) run evaluation on an independent test set

```{r exp_setup}
run.cv=TRUE # run cross-validation on the training set
K <- 3  # number of CV folds
run.test=TRUE # run evaluation on an independent test set
```



### Step 2: perform model selection by 5 folds cross-validation 
Using cross-validation or independent test set evaluation, we compare the performance of different classifiers or classifiers with different specifications. In this example, we use GBM with different `depth`. In the following chunk, we list, in a vector, setups (in this case, `depth`) corresponding to models that we will compare. In your project, you maybe comparing very different classifiers. You can assign them numerical IDs and labels specific to your project. 


```{r model_setup}
model_values <- seq(3, 11, 4)
model_labels = paste("GBM with depth =", model_values)
```



###  Read in data
```{r}
# lables(0 for muffin, 1 for chicken, 2 for dog)
labels <- read.csv("../data/label_train.csv",header=TRUE)
colnames(labels)[2] <- "labels"
sift_data <- read.csv("../data/sift_train.csv",header=TRUE, stringsAsFactors = FALSE)
```



```{r}
# prepare data
label_train <- labels[,-1]
dat_train <- sift_data[,-1]
```


### load train and test method
```{r loadlib}
source("../lib/baseline_train.R")
source("../lib/baseline_test.R")
```

#### Model selection with cross-validation
* Do model selection by choosing among different values of training model parameters, that is, the interaction depth for GBM in this example. 


```{r runcv, message=FALSE, warning=FALSE}
source("../lib/baseline_cv.R")

if(run.cv){
  err_cv <- array(dim=c(length(model_values), 2))
  for(k in 1:length(model_values)){
    cat("k=", k, "\n")
    err_cv[k,] <- cv.function(dat_train, label_train, model_values[k], K)
  }
  save(err_cv, file="../output/baseline_err_cv.RData")
}
```



### Step 3: Visualize cross-validation results. 

```{r cv_vis}
if(run.cv){
  load("../output/baseline_err_cv.RData")
  #pdf("../fig/cv_results.pdf", width=7, height=5)
  plot(model_values, err_cv[,1], xlab="Interaction Depth", ylab="CV Error",
       main="Cross Validation Error", type="n", ylim=c(0, 0.6))
  points(model_values, err_cv[,1], col="blue", pch=16)
  lines(model_values, err_cv[,1], col="blue")
  arrows(model_values, err_cv[,1]-err_cv[,2], model_values, err_cv[,1]+err_cv[,2], 
        length=0.1, angle=90, code=3)
  #dev.off()
}
```



* Choose the "best"" parameter value
```{r best_model}
model_best=model_values[1]
if(run.cv){
  model_best <- model_values[which.min(err_cv[,1])]
}

par_best <- list(depth=model_best)
cat(model_best)
```



* Train the model with the entire training set using the selected model (model parameter) via cross-validation.
```{r final_train}
tm_train=NA
tm_train <- system.time(fit_train <- train(dat_train, label_train, par_best))
save(fit_train, file="../output/baseline_fit_train.RData")
```



### Step 4: Make prediction 
Feed the final training model with the completely holdout testing data. 


#```{r}
#dat_test <- read.csv("../data/sift_test.csv", header=TRUE)
#dat_test <- dat_test[,-1] 
#```


#```{r test}
#tm_test=NA
#if(run.test){
#  load(file="../output/baseline_fit_train.RData")
#  tm_test <- system.time(pred_test <- test(fit_train, dat_test))
#  save(pred_test, file="../output/baseline_pred_test.RData")
#}
#```



### Summarize Running Time
Prediction performance matters, so does the running times for constructing features and for training the model, especially when the computation resource is limited. 
```{r running_time}
cat("Time for training model=", tm_train[1], "s \n")
#cat("Time for making prediction=", tm_test[1], "s \n")
```





